{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.model_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1286438640fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn.model_selection'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from math import log\n",
    "from math import exp\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import os;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8908604251539196\n"
     ]
    }
   ],
   "source": [
    "###### LOAD DATA ########\n",
    "dataPath = '/Users/Akshay/Dropbox/code/GenomeDreaming/data/'\n",
    "\n",
    "##Iterates through every FASTA file, parsing genome id (key) and sequence (value).\n",
    "genomes = []\n",
    "for fastaFile in os.listdir(dataPath):\n",
    "    if \"fna\" in fastaFile: #ensures only fasta files being read\n",
    "        handle = open(dataPath + fastaFile, \"r\")\n",
    "        genomes.append(list(SeqIO.parse(handle, \"fasta\")))\n",
    "        handle.close()\n",
    "len(genomes[1][0].seq)\n",
    "\n",
    "\n",
    "\n",
    "###### TRAIN N-gram Model ######\n",
    "train = genomes[1][0].seq\n",
    "test = \"GGAAA\"\n",
    "\n",
    "n = 1; #define the n for this n-gram\n",
    "\n",
    "#Laplacian smoothing for all combinations of n-gram; creates initial\n",
    "#frequency table with freq of 1 given to every possible combination.\n",
    "vocabulary = \"ATCG\"\n",
    "smoothingVocab = ['A','T','C','G']\n",
    "#perms = [''.join(p) for p in product(vocabulary,repeat = n)]\n",
    "bgs_smooth = nltk.ngrams(\"\",1)\n",
    "fdist = nltk.FreqDist(bgs_smooth)\n",
    "for p in product(vocabulary,repeat = n):\n",
    "    bgs_smooth = nltk.ngrams(p,n)\n",
    "    fdist += nltk.FreqDist(bgs_smooth)\n",
    "\n",
    "\n",
    "#Calculate actual n-gram frequencies in training set and add to \n",
    "#laplacian smoothing\n",
    "bgs_train = nltk.ngrams(train,n)\n",
    "fdist += nltk.FreqDist(bgs_train)\n",
    "\n",
    "#Convert n-gram frequencies to probabilities. |V| = 4^n is implicitly added to the denominator to account for laplacian smoothing,\n",
    "#since fdist.values already have 1 added to each |V|.\n",
    "totalVal = sum(fdist.values())\n",
    "for key in fdist.keys():\n",
    "    fdist[key] = fdist[key]/float(totalVal)\n",
    "#fdist.values()\n",
    "\n",
    "#Perplexity calculation. Remembe I need to add +V to bottom probability.\n",
    "#Used log probabilities, so log perplexity calculation since risk of underflow with lots of probabilities\n",
    "#Need to make sure I'm calculating the perplexity correctly for ngrams;\n",
    "#specifically, the idea of using the previous n words to predict the next\n",
    "#right now i'm just treating each consecutive n-gram in the test set\n",
    "#as independent from the last and multiplying all their probabilities\n",
    "#(sum here, because log)\n",
    "def perplexity(testset, model):\n",
    "    perplexity = 1\n",
    "    #iterate through all possible n-grams; won't work for n=1\n",
    "    for char in range(n, len(test)+1):\n",
    "        ngram = tuple(test[char-n:char])\n",
    "        perplexity = perplexity * (1/model.get(ngram))\n",
    "    perplexity = pow(perplexity, 1/float()) \n",
    "    return perplexity\n",
    "\n",
    "test = genomes[1][0].seq\n",
    "perplexity = 0\n",
    "##### CALCULATE TEST SEQUENCE PERPLEXITY #####\n",
    "for char in range(n, len(test)+1):\n",
    "    ngram = tuple(test[char-n:char])\n",
    "    perplexity += log(1/fdist.get(ngram))\n",
    "    #print(ngram)\n",
    "    #print(fdist.get(ngram))\n",
    "    #print(perplexity)\n",
    "perplexity = exp(perplexity/(len(test)-n+1)) \n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monty', 'the', 'hamburger']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perplexity with 5-gram\n",
    "805.0094611485741\n",
    "1140.0463786516896\n",
    "\n",
    "Perplexity w/ 2-gram\n",
    "14.880629212981228\n",
    "16.812607119054302\n",
    "\n",
    "Perplexity w/ 1-gram\n",
    "3.8908604251539196\n",
    "4.133334511155625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus =\"\"\"\n",
    "Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created the sketch comedy show Monty Python's Flying Circus,\n",
    "that first aired on the BBC on October 5, 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series\n",
    "into something larger in scope and impact, spawning touring stage shows, films, numerous albums, several books, and a stage musical.\n",
    "The group's influence on comedy has been compared to The Beatles' influence on music.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.83084577114428\n",
      "99.99999999999997\n"
     ]
    }
   ],
   "source": [
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "#here you construct the unigram language model \n",
    "def unigram(tokens):    \n",
    "    model = collections.defaultdict(lambda: 0.01) #0.01 is laplace smoothing factor\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model [f] = 1\n",
    "            continue\n",
    "    for word in model:\n",
    "        model[word] = model[word]/float(len(model))\n",
    "    return model\n",
    "\n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity\n",
    "\n",
    "testset1 = \"Monty\"\n",
    "testset2 = \"abracadabra gobbledygook rubbish\"\n",
    "\n",
    "model = unigram(tokens)\n",
    "print(perplexity(testset1, model))\n",
    "print(perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramModelVocabulary({'a': 4, 'd': 3, 'f': 3, 'b': 2, 'c': 2, ' ': 2, 's': 2, 'e': 1, 'g': 1, 'h': 1})\n",
      "[(' ',), ('<s>',), ('a',), ('b',), ('c',), ('d',), ('e',), ('f',), ('g',), ('h',), ('s',)]\n",
      "<FreqDist with 1 samples and 3 outcomes>\n",
      "<FreqDist with 12 samples and 63 outcomes>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from nltk.model import MLENgramModel\n",
    "from nltk.model import count_ngrams\n",
    "\n",
    "text=\"abcdefghabc asdf asdf\"\n",
    "#Create your bigrams\n",
    "bgs = nltk.ngrams(text,5)\n",
    "#five_model = MLENgramModel(bgs)\n",
    "vocab = build_vocabulary(1, text)\n",
    "print(vocab)\n",
    "bigram_counts = count_ngrams(2, vocab, text)\n",
    "print(sorted(bigram_counts.ngrams[2].conditions()))\n",
    "print(bigram_counts.ngrams[2][('f',)])\n",
    "vocab\n",
    "print(bigram_counts.unigrams)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "bigram_model = MLENgramModel(bigram_counts)\n",
    "ex_score = bigram_model.score(\"yawned\", [\"he\"])\n",
    "print(ex_score)\n",
    "\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "for k,v in fdist.items():\n",
    "   k\n",
    "\n",
    "testset1 = \"Monty\"\n",
    "testset2 = \"abracadabra gobbledygook rubbish\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-b76c348e7bd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-b76c348e7bd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [a b c]\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[a b c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram = []\n",
    "for Ngram in range (1,4):\n",
    "    gram.append(1)\n",
    "tuple(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-b6004ca98eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830138"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genomeDict = {};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
    "\n",
    "pickle.dump( favorite_color, open( \"save.p\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fav = pickle.load( open( \"save.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kitty': 'red', 'lion': 'yellow'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
