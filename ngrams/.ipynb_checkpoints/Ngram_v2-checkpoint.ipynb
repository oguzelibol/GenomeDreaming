{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ecol', 'ecg', 'eoc', 'elf', 'ena', 'ece', 'ebd', 'ecos', 'ecd', 'eoj', 'edh', 'eih', 'ecq', 'eso', 'esl', 'ecoo', 'ebe', 'elr', 'ecw', 'ecx', 'ecl', 'ecoi', 'eun', 'eum', 'ecoj', 'eld', 'etw', 'ecp', 'eok', 'ebw', 'ecf', 'elc', 'ecc', 'elh', 'ese', 'eal', 'ecm', 'elo', 'eoh', 'ell', 'edj', 'esm', 'eci', 'ecj', 'eck', 'ecok', 'ecz', 'ebl', 'ecr', 'ekf', 'ecs', 'elx', 'eoi', 'ecy', 'eco', 'ecoa', 'eab', 'eln', 'ecv', 'elp', 'ect', 'ecoh', 'efe', 'eko', 'elw', 'ebr', 'elu']\n",
      "['tpa', 'tph', 'tsu', 'tpp', 'taz', 'tbe', 'tpu', 'scd', 'tpk', 'tpw', 'tpc', 'tped', 'tpas', 'tpm', 'tde', 'tpg', 'tpo', 'tpb', 'trm', 'tpi', 'tpl']\n",
      "3.999641469978066\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from math import log\n",
    "from math import exp\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import os;\n",
    "\n",
    "import json\n",
    "from util import util\n",
    "###### LOAD DATA ########\n",
    "orgs = json.loads(open('y.json').read())\n",
    "util = util('prokaryotes/')\n",
    "        \n",
    "##Iterates through every FASTA file, parsing genome id (key) and sequence (value).\n",
    "orgs[\"eco\"][\"class\"]\n",
    "list(orgs.keys())[0]\n",
    "Escherichia = list()\n",
    "Treponema = list()\n",
    "for org in orgs.keys():\n",
    "    if(orgs[org][\"class\"] == \"Escherichia\"):\n",
    "        Escherichia.append(org);\n",
    "    if(orgs[org][\"class\"] == \"Treponema\"):\n",
    "        Treponema.append(org);\n",
    "print(Escherichia)\n",
    "print(Treponema)\n",
    "\n",
    "###### TRAIN N-gram Model ######\n",
    "n_max = 2; #define the n for this n-gram\n",
    "\n",
    "#Laplacian smoothing for all combinations of n-gram; creates initial\n",
    "#frequency table with freq of var smooth given to every possible combination.\n",
    "smooth = 0.1\n",
    "vocabulary = \"ATCG\"\n",
    "smoothingVocab = ['A','T','C','G']\n",
    "#perms = [''.join(p) for p in product(vocabulary,repeat = n)]\n",
    "train = util.get_single_org_genome(Escherichia[1]+'.fna')\n",
    "bg_smooth = nltk.ngrams(\"\",1)\n",
    "fdistTot = nltk.FreqDist(bg_smooth)\n",
    "\n",
    "for n in range(1,n_max):\n",
    "    bgs_smooth = nltk.ngrams(\"\",1)\n",
    "    fdist = nltk.FreqDist(bgs_smooth)\n",
    "    for p in product(vocabulary,repeat = n):\n",
    "        bgs_smooth = nltk.ngrams(p,n)\n",
    "        fdist += nltk.FreqDist(bgs_smooth)\n",
    "    for key,value in fdist.items(): \n",
    "        fdist[key] = value*smooth  \n",
    "\n",
    "    #Calculate actual n-gram frequencies in training set and add to \n",
    "    #laplacian smoothing\n",
    "    #for i in range(1,len(Escherichia)):\n",
    "     #   print(Escherichia[i])\n",
    "    bgs_train = nltk.ngrams(train,n)\n",
    "    fdist += nltk.FreqDist(bgs_train)\n",
    "    #Convert n-gram frequencies to probabilities. |V| = 4^n is implicitly added to the denominator to account for laplacian smoothing,\n",
    "    #since fdist.values already have 1 added to each |V|.\n",
    "    totalVal = sum(fdist.values())\n",
    "    for key in fdist.keys():\n",
    "        fdist[key] = fdist[key]/float(totalVal)\n",
    "    \n",
    "    fdistTot += fdist\n",
    "\n",
    "def perplexity(test, model):\n",
    "    perp = 1\n",
    "    #iterate through all possible n-grams; won't work for n=1\n",
    "    for char in range(n, len(test)+1):\n",
    "        ngram = tuple(test[char-n:char])\n",
    "       #print(ngram);\n",
    "        perp = perp + log(1/model.get(ngram))\n",
    "    perp = exp(perp/(len(test)+1-n)) \n",
    "    return perp\n",
    "\n",
    "\n",
    "##### CALCULATE TEST SEQUENCE PERPLEXITY #####\n",
    "test = util.get_single_org_genome(Escherichia[1]+'.fna')\n",
    "perp = perplexity(test,fdist)\n",
    "print(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monty', 'the', 'hamburger']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Getting exponentially increasing perplexity because of laplacian smoothing. Not enough data to make it useful for the larger grams. Try without\n",
    "##or just add a smaller factor? Like 0.01 to each instead of 1? \n",
    "915.5344545914832\n",
    "Perplexity with 5-gram\n",
    "805.0094611485741\n",
    "1140.0463786516896\n",
    "\n",
    "Perplexity w/ 2-gram\n",
    "14.880629212981228\n",
    "16.812607119054302\n",
    "\n",
    "Perplexity w/ 1-gram\n",
    "3.8908604251539196\n",
    "4.133334511155625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus =\"\"\"\n",
    "Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created the sketch comedy show Monty Python's Flying Circus,\n",
    "that first aired on the BBC on October 5, 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series\n",
    "into something larger in scope and impact, spawning touring stage shows, films, numerous albums, several books, and a stage musical.\n",
    "The group's influence on comedy has been compared to The Beatles' influence on music.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.83084577114428\n",
      "99.99999999999997\n"
     ]
    }
   ],
   "source": [
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "#here you construct the unigram language model \n",
    "def unigram(tokens):    \n",
    "    model = collections.defaultdict(lambda: 0.01) #0.01 is laplace smoothing factor\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model [f] = 1\n",
    "            continue\n",
    "    for word in model:\n",
    "        model[word] = model[word]/float(len(model))\n",
    "    return model\n",
    "\n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity\n",
    "\n",
    "testset1 = \"Monty\"\n",
    "testset2 = \"abracadabra gobbledygook rubbish\"\n",
    "\n",
    "model = unigram(tokens)\n",
    "print(perplexity(testset1, model))\n",
    "print(perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramModelVocabulary({'a': 4, 'd': 3, 'f': 3, 'b': 2, 'c': 2, ' ': 2, 's': 2, 'e': 1, 'g': 1, 'h': 1})\n",
      "[(' ',), ('<s>',), ('a',), ('b',), ('c',), ('d',), ('e',), ('f',), ('g',), ('h',), ('s',)]\n",
      "<FreqDist with 1 samples and 3 outcomes>\n",
      "<FreqDist with 12 samples and 63 outcomes>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from nltk.model import MLENgramModel\n",
    "from nltk.model import count_ngrams\n",
    "\n",
    "text=\"abcdefghabc asdf asdf\"\n",
    "#Create your bigrams\n",
    "bgs = nltk.ngrams(text,5)\n",
    "#five_model = MLENgramModel(bgs)\n",
    "vocab = build_vocabulary(1, text)\n",
    "print(vocab)\n",
    "bigram_counts = count_ngrams(2, vocab, text)\n",
    "print(sorted(bigram_counts.ngrams[2].conditions()))\n",
    "print(bigram_counts.ngrams[2][('f',)])\n",
    "vocab\n",
    "print(bigram_counts.unigrams)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "bigram_model = MLENgramModel(bigram_counts)\n",
    "ex_score = bigram_model.score(\"yawned\", [\"he\"])\n",
    "print(ex_score)\n",
    "\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "for k,v in fdist.items():\n",
    "   k\n",
    "\n",
    "testset1 = \"Monty\"\n",
    "testset2 = \"abracadabra gobbledygook rubbish\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-b76c348e7bd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-b76c348e7bd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [a b c]\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[a b c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram = []\n",
    "for Ngram in range (1,4):\n",
    "    gram.append(1)\n",
    "tuple(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genomeDict = {};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import util\n",
    "import json\n",
    "orgs = json.loads(open('training_data/y.json').read())\n",
    "util = util('prokaryotes/')\n",
    "orgs[\"eco\"][\"class\"]\n",
    "list(orgs.keys())[0]\n",
    "for org in orgs.keys():\n",
    "    if(orgs[org][\"class\"] == \"Escherichia\"):\n",
    "        print(len(util.get_single_org_genome(org)));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(genomes[1][0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(util.get_single_org_genome(Escherichia[52]+'.fna'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tpc', 'tpu', 'trm', 'tpw', 'tsu', 'tde', 'tbe', 'tpa', 'tpg', 'tpm', 'tpl', 'tpi', 'taz', 'tped', 'tpo', 'tph', 'tpk', 'tpas', 'scd', 'tpb', 'tpp']\n"
     ]
    }
   ],
   "source": [
    "from util import util\n",
    "###### LOAD DATA ########\n",
    "orgs = json.loads(open('training_data/y.json').read())\n",
    "util = util('prokaryotes/')\n",
    "        \n",
    "##Iterates through every FASTA file, parsing genome id (key) and sequence (value).\n",
    "orgs[\"eco\"][\"class\"]\n",
    "list(orgs.keys())[0]\n",
    "Escherichia = list()\n",
    "Treponema = list()\n",
    "for org in orgs.keys():\n",
    "    if(orgs[org][\"class\"] == \"Escherichia\"):\n",
    "        Escherichia.append(org);\n",
    "    if(orgs[org][\"class\"] == \"Treponema\"):\n",
    "        Treponema.append(org);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4059867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tpi'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(util.get_single_org_genome(Treponema[11]+'.fna')))\n",
    "Treponema[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
